{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with DNA2Vec to create Sequence Embeddings\n",
    "## Ng et. al 2017\n",
    "We will first test this approach with the DNA2Vec to see if it has any merit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Not from Patrick Ng. Really awsome permutation code I found online.\n",
    "def SeqPermuteVocab(k_low, k_high, letters = 'ATCG'):\n",
    "    vocab = []\n",
    "    for i in range(k_low, k_high+1):\n",
    "        for seq in product(letters, repeat=i):\n",
    "            kmer = ''.join(seq)\n",
    "            vocab.append(kmer)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attic ####\n",
    "\n",
    "## Utils ##\n",
    "import random\n",
    "import string\n",
    "import resource\n",
    "import logbook\n",
    "import arrow\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def split_Xy(df, y_colname='label'):\n",
    "    X = df.drop([y_colname], axis=1)\n",
    "    y = df[y_colname]\n",
    "    return (X, y)\n",
    "\n",
    "def shuffle_tuple(tup, rng):\n",
    "    lst = list(tup)\n",
    "    rng.shuffle(lst)\n",
    "    return tuple(lst)\n",
    "\n",
    "def shuffle_dataframe(df, rng):\n",
    "    \"\"\"\n",
    "    this does NOT do in-place shuffling\n",
    "    \"\"\"\n",
    "    return df.reindex(rng.permutation(df.index))\n",
    "\n",
    "def random_str(N):\n",
    "    return ''.join(random.SystemRandom().choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(N))\n",
    "\n",
    "def memory_usage():\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1E6\n",
    "\n",
    "def estimate_bytes(filenames):\n",
    "    return sum([os.stat(f).st_size for f in filenames])\n",
    "\n",
    "def get_output_fileroot(dirpath, name, postfix):\n",
    "    return '{}/{}-{}-{}-{}'.format(\n",
    "        dirpath,\n",
    "        name,\n",
    "        arrow.utcnow().format('YYYYMMDD-HHmm'),\n",
    "        postfix,\n",
    "random_str(3))\n",
    "\n",
    "## Time Benchmark ##\n",
    "import time\n",
    "import logbook\n",
    "\n",
    "class Benchmark():\n",
    "    def __init__(self):\n",
    "        self.time_wall_start = time.time()\n",
    "        self.time_cpu_start = time.process_time()\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "\n",
    "    def diff_time_wall_secs(self):\n",
    "        return (time.time() - self.time_wall_start)\n",
    "\n",
    "    def print_time(self, label=''):\n",
    "        self.logger.info(\"%s wall=%.3fm cpu=%.3fm\" % (\n",
    "            label,\n",
    "            self.diff_time_wall_secs() / 60.0,\n",
    "            (time.clock() - self.time_cpu_start) / 60.0,\n",
    "        ))\n",
    "\n",
    "## Tee ##\n",
    "import sys\n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, fptr):\n",
    "        self.file = fptr\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        sys.stdout = self.stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "\n",
    "## Bio Util ##\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "NUCLEOTIDES = 'ACGT'\n",
    "\n",
    "class Tuple4:\n",
    "    def __init__(self, pos1, pos2, neg1, neg2):\n",
    "        self.pos1 = pos1\n",
    "        self.pos2 = pos2\n",
    "        self.neg1 = neg1\n",
    "        self.neg2 = neg2\n",
    "\n",
    "def determine_out_filename(output_dir, fileroot, mode, extension='txt'):\n",
    "    return os.path.join(output_dir, '{}.{}.{}'.format(fileroot, mode, extension))\n",
    "\n",
    "def create_tuple4(kmer1, kmer2, kmer1_neg, kmer2_neg):\n",
    "    \"\"\"\n",
    "    all inputs are list of single nucleotides, e.g. ['A', 'A', 'C']\n",
    "    \"\"\"\n",
    "    return Tuple4(\n",
    "        ''.join(kmer1),\n",
    "        ''.join(kmer2),\n",
    "        ''.join(kmer1_neg),\n",
    "        ''.join(kmer2_neg))\n",
    "\n",
    "def insert_snippet(seq, snippet, idx):\n",
    "    \"\"\"\n",
    "    idx: 0 <= idx <= len(seq)]\n",
    "    \"\"\"\n",
    "    split1 = seq[:idx]\n",
    "    split2 = seq[idx:]\n",
    "    return split1 + snippet + split2\n",
    "\n",
    "def pairwise_key(v1, v2):\n",
    "    return '{}:{}'.format(v1, v2)\n",
    "\n",
    "def rand_kmer(rng, k_low, k_high=None):\n",
    "    \"\"\"\n",
    "    k_low and k_high are inclusive\n",
    "    \"\"\"\n",
    "    if k_high is None:\n",
    "        k_high = k_low\n",
    "    k_len = rng.randint(k_low, k_high + 1)\n",
    "    return ''.join([NUCLEOTIDES[x] for x in rng.randint(4, size=k_len)])\n",
    "\n",
    "def rand_nt(rng):\n",
    "    return NUCLEOTIDES[rng.randint(4)]\n",
    "\n",
    "def generate_revcompl_pair(k_low, k_high=None, rng=None):\n",
    "    # TODO make params k_high and rng be required\n",
    "    if k_high is None:\n",
    "        k_high = k_low\n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "    kmer = rand_kmer(rng, k_low, k_high)\n",
    "    return (kmer, revcompl(kmer))\n",
    "\n",
    "def revcompl(kmer):\n",
    "    return str(Seq(kmer).reverse_complement())\n",
    "\n",
    "def generate_1nt_mutation_4tuple(rng, k_len):\n",
    "    kmer1 = list(rand_kmer(rng, k_len, k_len))\n",
    "    kmer2 = list(rand_kmer(rng, k_len, k_len))\n",
    "\n",
    "    idx = rng.randint(len(kmer1))\n",
    "    original_nt = kmer1[idx]\n",
    "    mutate_nt = rand_nt(rng)\n",
    "\n",
    "    kmer1_neg = list(kmer1)\n",
    "    kmer1_neg[idx] = mutate_nt\n",
    "\n",
    "    kmer2[idx] = mutate_nt\n",
    "    kmer2_neg = list(kmer2)\n",
    "    kmer2_neg[idx] = original_nt\n",
    "\n",
    "    return create_tuple4(kmer1, kmer2, kmer1_neg, kmer2_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generators ####\n",
    "import logbook\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def remove_empty(str_list):\n",
    "    return filter(bool, str_list)  # fastest way to remove empty string\n",
    "\n",
    "class SeqFragmenter:\n",
    "    \"\"\"\n",
    "    Split a sequence into small sequences based on some criteria, e.g. 'N' characters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_acgt_seqs(self, seq):\n",
    "        return remove_empty(re.split(r'[^ACGTacgt]+', str(seq)))\n",
    "\n",
    "class SlidingKmerFragmenter:\n",
    "    \"\"\"\n",
    "    Slide only a single nucleotide\n",
    "    \"\"\"\n",
    "    def __init__(self, k_low, k_high):\n",
    "        self.k_low = k_low\n",
    "        self.k_high = k_high\n",
    "\n",
    "    def apply(self, rng, seq):\n",
    "        return [seq[i: i + rng.randint(self.k_low, self.k_high + 1)] for i in range(len(seq) - self.k_high + 1)]\n",
    "\n",
    "class DisjointKmerFragmenter:\n",
    "    \"\"\"\n",
    "    Split a sequence into kmers\n",
    "    \"\"\"\n",
    "    def __init__(self, k_low, k_high):\n",
    "        self.k_low = k_low\n",
    "        self.k_high = k_high\n",
    "\n",
    "    @staticmethod\n",
    "    def random_chunks(rng, li, min_chunk, max_chunk):\n",
    "        \"\"\"\n",
    "        Both min_chunk and max_chunk are inclusive\n",
    "        \"\"\"\n",
    "        it = iter(li)\n",
    "        while True:\n",
    "            head_it = islice(it, rng.randint(min_chunk, max_chunk + 1))\n",
    "            nxt = '' . join(head_it)\n",
    "\n",
    "            # throw out chunks that are not within the kmer range\n",
    "            if len(nxt) >= min_chunk:\n",
    "                yield nxt\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def apply(self, rng, seq):\n",
    "        seq = seq[rng.randint(self.k_low):]  # randomly offset the beginning to create more variations\n",
    "        return list(DisjointKmerFragmenter.random_chunks(rng, seq, self.k_low, self.k_high))\n",
    "\n",
    "class SeqMapper:\n",
    "    def __init__(self, use_revcomp=True):\n",
    "        self.use_revcomp = use_revcomp\n",
    "\n",
    "    def apply(self, rng, seq):\n",
    "        seq = seq.upper()\n",
    "        if self.use_revcomp and rng.rand() < 0.5:\n",
    "            return seq.reverse_complement()\n",
    "        else:\n",
    "            return seq\n",
    "\n",
    "class SeqGenerator:\n",
    "    def __init__(self, filenames, nb_epochs, seqlen_ulim=5000):\n",
    "        self.filenames = filenames\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.seqlen_ulim = seqlen_ulim\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "        self.logger.info('Number of epochs: {}'.format(nb_epochs))\n",
    "\n",
    "    def filehandle_generator(self):\n",
    "        for curr_epoch in range(self.nb_epochs):\n",
    "            for filename in self.filenames:\n",
    "                with open(filename) as file:\n",
    "                    self.logger.info('Opened file: {}'.format(filename))\n",
    "                    self.logger.info('Memory usage: {} MB'.format(memory_usage()))\n",
    "                    self.logger.info('Current epoch: {} / {}'.format(curr_epoch + 1, self.nb_epochs))\n",
    "                    yield file\n",
    "\n",
    "    def generator(self, rng):\n",
    "        for fh in self.filehandle_generator():\n",
    "            # SeqIO takes twice as much memory than even simple fh.readlines()\n",
    "            for seq_record in SeqIO.parse(fh, \"fasta\"):\n",
    "                whole_seq = seq_record.seq\n",
    "                self.logger.info('Whole fasta seqlen: {}'.format(len(whole_seq)))\n",
    "                curr_left = 0\n",
    "                while curr_left < len(whole_seq):\n",
    "                    seqlen = rng.randint(self.seqlen_ulim // 2, self.seqlen_ulim)\n",
    "                    segment = seq_record.seq[curr_left: seqlen + curr_left]\n",
    "                    curr_left += seqlen\n",
    "                    self.logger.debug('input seq len: {}'.format(len(segment)))\n",
    "                    yield segment\n",
    "\n",
    "class KmerSeqIterable:\n",
    "    def __init__(self, rand_seed, seq_generator, mapper, seq_fragmenter, kmer_fragmenter, histogram):\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "        self.seq_generator = seq_generator\n",
    "        self.mapper = mapper\n",
    "        self.kmer_fragmenter = kmer_fragmenter\n",
    "        self.seq_fragmenter = seq_fragmenter\n",
    "        self.histogram = histogram\n",
    "        self.rand_seed = rand_seed\n",
    "        self.iter_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iter_count += 1\n",
    "        rng = np.random.RandomState(self.rand_seed)\n",
    "        for seq in self.seq_generator.generator(rng):\n",
    "            seq = self.mapper.apply(rng, seq)\n",
    "            acgt_seq_splits = list(self.seq_fragmenter.get_acgt_seqs(seq))\n",
    "            self.logger.debug('Splits of len={} to: {}'.format(len(seq), [len(f) for f in acgt_seq_splits]))\n",
    "\n",
    "            for acgt_seq in acgt_seq_splits:\n",
    "                kmer_seq = self.kmer_fragmenter.apply(rng, acgt_seq)  # list of strings\n",
    "                if len(kmer_seq) > 0:\n",
    "                    if self.iter_count == 1:\n",
    "                        # only collect stats on the first call\n",
    "                        self.histogram.add(kmer_seq)\n",
    "                    yield kmer_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram ###\n",
    "from collections import Counter\n",
    "import logbook\n",
    "\n",
    "class Histogram:\n",
    "    def __init__(self):\n",
    "        self.kmer_len_counter = Counter()\n",
    "        self.nb_kmers = 0\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "\n",
    "    def add(self, seq):\n",
    "        \"\"\"\n",
    "        seq - array of k-mer string\n",
    "        \"\"\"\n",
    "        for kmer in seq:\n",
    "            self.kmer_len_counter[len(kmer)] += 1\n",
    "            self.nb_kmers += 1\n",
    "\n",
    "    def print_stat(self, fptr):\n",
    "        for kmer_len in sorted(self.kmer_len_counter.keys()):\n",
    "            self.logger.info('Percent of {:2d}-mers: {:3.1f}% ({})'.format(\n",
    "                kmer_len,\n",
    "                100.0 * self.kmer_len_counter[kmer_len] / self.nb_kmers,\n",
    "                self.kmer_len_counter[kmer_len],\n",
    "            ))\n",
    "\n",
    "        total_bps = sum([l * c for l, c in self.kmer_len_counter.items()])\n",
    "        self.logger.info('Number of base-pairs: {}'.format(total_bps))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "NUCLEOTIDES = 'ACGT'\n",
    "\n",
    "class Tuple4:\n",
    "    def __init__(self, pos1, pos2, neg1, neg2):\n",
    "        self.pos1 = pos1\n",
    "        self.pos2 = pos2\n",
    "        self.neg1 = neg1\n",
    "        self.neg2 = neg2\n",
    "\n",
    "def determine_out_filename(output_dir, fileroot, mode, extension='txt'):\n",
    "    return os.path.join(output_dir, '{}.{}.{}'.format(fileroot, mode, extension))\n",
    "\n",
    "def create_tuple4(kmer1, kmer2, kmer1_neg, kmer2_neg):\n",
    "    \"\"\"\n",
    "    all inputs are list of single nucleotides, e.g. ['A', 'A', 'C']\n",
    "    \"\"\"\n",
    "    return Tuple4(\n",
    "        ''.join(kmer1),\n",
    "        ''.join(kmer2),\n",
    "        ''.join(kmer1_neg),\n",
    "        ''.join(kmer2_neg))\n",
    "\n",
    "def insert_snippet(seq, snippet, idx):\n",
    "    \"\"\"\n",
    "    idx: 0 <= idx <= len(seq)]\n",
    "    \"\"\"\n",
    "    split1 = seq[:idx]\n",
    "    split2 = seq[idx:]\n",
    "    return split1 + snippet + split2\n",
    "\n",
    "def pairwise_key(v1, v2):\n",
    "    return '{}:{}'.format(v1, v2)\n",
    "\n",
    "def rand_kmer(rng, k_low, k_high=None):\n",
    "    \"\"\"\n",
    "    k_low and k_high are inclusive\n",
    "    \"\"\"\n",
    "    if k_high is None:\n",
    "        k_high = k_low\n",
    "    k_len = rng.randint(k_low, k_high + 1)\n",
    "    return ''.join([NUCLEOTIDES[x] for x in rng.randint(4, size=k_len)])\n",
    "\n",
    "def rand_nt(rng):\n",
    "    return NUCLEOTIDES[rng.randint(4)]\n",
    "\n",
    "def generate_revcompl_pair(k_low, k_high=None, rng=None):\n",
    "    # TODO make params k_high and rng be required\n",
    "    if k_high is None:\n",
    "        k_high = k_low\n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "    kmer = rand_kmer(rng, k_low, k_high)\n",
    "    return (kmer, revcompl(kmer))\n",
    "\n",
    "def revcompl(kmer):\n",
    "    return str(Seq(kmer).reverse_complement())\n",
    "\n",
    "def generate_1nt_mutation_4tuple(rng, k_len):\n",
    "    kmer1 = list(rand_kmer(rng, k_len, k_len))\n",
    "    kmer2 = list(rand_kmer(rng, k_len, k_len))\n",
    "\n",
    "    idx = rng.randint(len(kmer1))\n",
    "    original_nt = kmer1[idx]\n",
    "    mutate_nt = rand_nt(rng)\n",
    "\n",
    "    kmer1_neg = list(kmer1)\n",
    "    kmer1_neg[idx] = mutate_nt\n",
    "\n",
    "    kmer2[idx] = mutate_nt\n",
    "    kmer2_neg = list(kmer2)\n",
    "    kmer2_neg[idx] = original_nt\n",
    "\n",
    "    return create_tuple4(kmer1, kmer2, kmer1_neg, kmer2_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarkana/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "### Multi_K Model ###\n",
    "from __future__ import print_function\n",
    "\n",
    "import logbook\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import matutils\n",
    "\n",
    "class SingleKModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.vocab_lst = sorted(model.vocab.keys())\n",
    "\n",
    "class MultiKModel:\n",
    "    def __init__(self, filepath):\n",
    "        self.aggregate = word2vec.Word2Vec.load_word2vec_format(filepath, binary=False)\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "        vocab_lens = [len(vocab) for vocab in self.aggregate.vocab.keys()]\n",
    "        voc_vec = word2vec.Word2Vec(vocab, min_count=1)\n",
    "        self.k_low = min(vocab_lens)\n",
    "        self.k_high = max(vocab_lens)\n",
    "        self.vec_dim = self.aggregate.vector_size\n",
    "\n",
    "        self.data = {}\n",
    "        for k in range(self.k_low, self.k_high + 1):\n",
    "            self.data[k] = self.separate_out_model(k)\n",
    "\n",
    "    def model(self, k_len):\n",
    "        \"\"\"\n",
    "        Use vector('ACGTA') when possible\n",
    "        \"\"\"\n",
    "        return self.data[k_len].model\n",
    "\n",
    "    def vector(self, vocab):\n",
    "        return self.data[len(vocab)].model[vocab]\n",
    "\n",
    "    def unitvec(self, vec):\n",
    "        return matutils.unitvec(vec)\n",
    "\n",
    "    def cosine_distance(self, vocab1, vocab2):\n",
    "        return np.dot(self.unitvec(self.vector(vocab1)), self.unitvec(self.vector(vocab2)))\n",
    "\n",
    "    def l2_norm(self, vocab):\n",
    "        return np.linalg.norm(self.vector(vocab))\n",
    "\n",
    "    def separate_out_model(self, k_len):\n",
    "        vocabs = [vocab for vocab in self.aggregate.vocab.keys() if len(vocab) == k_len]\n",
    "        if len(vocabs) != 4 ** k_len:\n",
    "            self.logger.warn('Missing {}-mers: {} / {}'.format(k_len, len(vocabs), 4 ** k_len))\n",
    "\n",
    "        header_str = '{} {}'.format(len(vocabs), self.vec_dim)\n",
    "        with tempfile.NamedTemporaryFile(mode='w') as fptr:\n",
    "            print(header_str, file=fptr)\n",
    "            for vocab in vocabs:\n",
    "                vec_str = ' '.join(\"%f\" % val for val in self.aggregate[vocab])\n",
    "                print('{} {}'.format(vocab, vec_str), file=fptr)\n",
    "            fptr.flush()\n",
    "            return SingleKModel(word2vec.Word2Vec.load_word2vec_format(fptr.name, binary=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/celeganstest/dna2vec-20190610-2141-k50to50-12d-4c-206Mbp-sliding-D3f.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4653330bbef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4653330bbef4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mout_txt_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_fileroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_txt_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msummary_fptr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_fptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mlogbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_application\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/celeganstest/dna2vec-20190610-2141-k50to50-12d-4c-206Mbp-sliding-D3f.txt'"
     ]
    }
   ],
   "source": [
    "### Modified Execution Script ###\n",
    "import glob\n",
    "import logbook\n",
    "from logbook.compat import redirect_logging\n",
    "import configargparse\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "#Creating Arguements Class\n",
    "\n",
    "class arguements:\n",
    "    def __init__(self, kmer_fragmenter, vec_dim, rseed, rseed_trainset, inputs,\n",
    "                 k_low, k_high, context, epochs, gensim_iters, out_dir, debug):\n",
    "        self.kmer_fragmenter = kmer_fragmenter\n",
    "        self.vec_dim = vec_dim\n",
    "        self.rseed = rseed\n",
    "        self.rseed_trainset = rseed_trainset\n",
    "        self.inputs = inputs\n",
    "        self.k_low = k_low\n",
    "        self.k_high = k_high\n",
    "        self.context = context\n",
    "        self.epochs = epochs\n",
    "        self.gensim_iters = gensim_iters\n",
    "        self.out_dir = out_dir\n",
    "        self.debug = debug\n",
    "#Setting Arguements for code\n",
    "args = arguements(kmer_fragmenter = 'sliding', inputs = '../data/c_elegans_testset/*',k_low = 50,\n",
    "                  k_high = 50, vec_dim = 100, epochs = 2, context = 4,\n",
    "                  out_dir='../data/c', rseed = 7, rseed_trainset=123,\n",
    "                 gensim_iters = 1, debug = False)\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#Learner is currently using skipgram, but could change to cbow\n",
    "class Learner:\n",
    "    def __init__(self, out_fileroot, context_halfsize, gensim_iters, vec_dim):\n",
    "        self.logger = logbook.Logger(self.__class__.__name__)\n",
    "        assert(word2vec.FAST_VERSION >= 0)\n",
    "        self.logger.info('word2vec.FAST_VERSION (should be >= 0): {}'.format(word2vec.FAST_VERSION))\n",
    "        self.model = None\n",
    "        self.out_fileroot = out_fileroot\n",
    "        self.context_halfsize = context_halfsize\n",
    "        self.gensim_iters = gensim_iters\n",
    "        self.use_skipgram = 1\n",
    "        self.vec_dim = vec_dim\n",
    "\n",
    "        self.logger.info('Context window half size: {}'.format(self.context_halfsize))\n",
    "        self.logger.info('Use Skip Gram: {}'.format(self.use_skipgram))\n",
    "        self.logger.info('gensim_iters: {}'.format(self.gensim_iters))\n",
    "        self.logger.info('vec_dim: {}'.format(self.vec_dim))\n",
    "\n",
    "    def train(self, kmer_seq_generator):\n",
    "        self.model = word2vec.Word2Vec(\n",
    "            sentences=kmer_seq_generator,\n",
    "            size=self.vec_dim,\n",
    "            window=self.context_halfsize,\n",
    "            min_count=5,\n",
    "            workers=4,\n",
    "            sg=self.use_skipgram,\n",
    "            iter=self.gensim_iters)\n",
    "\n",
    "        # self.logger.info(model.vocab)\n",
    "\n",
    "    def write_vec(self):\n",
    "        out_filename = '{}.w2v'.format(self.out_fileroot)\n",
    "        self.model.wv.save_word2vec_format(out_filename, binary=False)\n",
    "\n",
    "def run_main(args, inputs, out_fileroot):\n",
    "    logbook.info(' '.join(sys.argv))\n",
    "    if not args.debug:\n",
    "        import logging\n",
    "        logging.getLogger('gensim.models.word2vec').setLevel(logging.INFO)\n",
    "\n",
    "    np.random.seed(args.rseed)\n",
    "\n",
    "    benchmark = Benchmark()\n",
    "\n",
    "    if args.kmer_fragmenter == 'disjoint':\n",
    "        kmer_fragmenter = DisjointKmerFragmenter(args.k_low, args.k_high)\n",
    "    elif args.kmer_fragmenter == 'sliding':\n",
    "        kmer_fragmenter = SlidingKmerFragmenter(args.k_low, args.k_high)\n",
    "    else:\n",
    "        raise InvalidArgException('Invalid kmer fragmenter: {}'.format(args.kmer_fragmenter))\n",
    "\n",
    "    logbook.info('kmer fragmenter: {}'.format(args.kmer_fragmenter))\n",
    "\n",
    "    histogram = Histogram()\n",
    "    kmer_seq_iterable = KmerSeqIterable(\n",
    "        args.rseed_trainset,\n",
    "        SeqGenerator(inputs, args.epochs),\n",
    "        SeqMapper(),\n",
    "        SeqFragmenter(),\n",
    "        kmer_fragmenter,\n",
    "        histogram,\n",
    "    )\n",
    "\n",
    "    learner = Learner(out_fileroot, args.context, args.gensim_iters, args.vec_dim)\n",
    "    learner.train(kmer_seq_iterable)\n",
    "    learner.write_vec()\n",
    "\n",
    "    histogram.print_stat(sys.stdout)\n",
    "\n",
    "    benchmark.print_time()\n",
    "\n",
    "def main():\n",
    "    if args.debug:\n",
    "        out_dir = '/tmp'\n",
    "        log_level = 'DEBUG'\n",
    "    else:\n",
    "        out_dir = args.out_dir\n",
    "        log_level = 'INFO'\n",
    "\n",
    "    inputs = glob.glob(args.inputs)\n",
    "\n",
    "    mbytes = estimate_bytes(inputs) // (10 ** 6)\n",
    "    out_fileroot = get_output_fileroot(\n",
    "        out_dir,\n",
    "        'dna2vec',\n",
    "        'k{}to{}-{}d-{}c-{}Mbp-{}'.format(\n",
    "            args.k_low,\n",
    "            args.k_high,\n",
    "            args.vec_dim,\n",
    "            args.context,\n",
    "            mbytes * args.epochs,  # total Mb including epochs\n",
    "            args.kmer_fragmenter))\n",
    "\n",
    "    out_txt_filename = '{}.txt'.format(out_fileroot)\n",
    "    with open(out_txt_filename, 'w') as summary_fptr:\n",
    "        with Tee(summary_fptr):\n",
    "            logbook.StreamHandler(sys.stdout, level=log_level).push_application()\n",
    "            redirect_logging()\n",
    "            run_main(args, inputs, out_fileroot)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
